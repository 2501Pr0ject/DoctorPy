com" ## Proxies¶ urllib will auto-detect your proxy settings and use those. This is through the ProxyHandler, which is part of the normal handler chain when a proxy setting is detected. Normally that’s a good thing, but there are occasions when it may not be helpful [5]. One way to do this is to setup our own ProxyHandler, with no proxies defined. This is done using similar steps to setting up a Basic Authentication handler: ProxyHandler ProxyHandler >>> proxy_support = urllib. request. ProxyHandler({}) >>> opener = urllib. request. build_opener(proxy_support) >>> urllib. request. install_opener(opener) Note Currently urllib. request does not support fetching of https locations through a proxy. However, this can be enabled by extending urllib. request as shown in the recipe [6]. urllib. request https Note HTTP_PROXY will be ignored if a variable REQUEST_METHOD is set; see the documentation on getproxies(). HTTP_PROXY REQUEST_METHOD getproxies() ## Sockets and Layers¶ The Python support for fetching resources from the web is layered. urllib uses the http. client library, which in turn uses the socket library. http. client As of Python 2. 3 you can specify how long a socket should wait for a response before timing out. This can be useful in applications which have to fetch web pages. By default the socket module has no timeout and can hang. Currently, the socket timeout is not exposed at the http. client or urllib. request levels. However, you can set the default timeout globally for all sockets using import socket import urllib. request # timeout in seconds timeout = 10 socket. setdefaulttimeout(timeout) # this call to urllib. request. urlopen now uses the default timeout # we have set in the socket module req = urllib. request. Request(' response = urllib. request. urlopen(req) ## Footnotes¶ This document was reviewed and revised by John Lee.