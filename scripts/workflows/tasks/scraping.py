"""
TÃ¢ches Prefect pour le scraping de donnÃ©es
"""

import sys
from pathlib import Path
from typing import List, Dict, Any
from prefect import task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner

# Ajouter le rÃ©pertoire racine au path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from scripts.data_processing.scrape_docs import DocumentScraper


@task(
    name="scrape_python_docs",
    description="Scraper la documentation Python officielle",
    retries=3,
    retry_delay_seconds=[60, 180, 300],  # Backoff progressif
    timeout_seconds=1800,  # 30 minutes max
    tags=["data", "scraping", "python-docs"]
)
async def scrape_python_docs(
    force_refresh: bool = False,
    max_documents: int = None
) -> Dict[str, Any]:
    """
    Scraper la documentation Python avec gestion d'erreurs robuste
    
    Args:
        force_refresh: Force le re-scraping mÃªme si les documents existent
        max_documents: Limite le nombre de documents (pour tests)
        
    Returns:
        Dict avec les mÃ©tadonnÃ©es du scraping
    """
    logger = get_run_logger()
    
    try:
        logger.info("ğŸ•·ï¸ DÃ©marrage du scraping de la documentation Python")
        
        # Initialiser le scraper
        scraper = DocumentScraper()
        
        # Configuration conditionnelle pour les tests
        if max_documents:
            logger.info(f"Mode test : limitation Ã  {max_documents} documents")
            scraper.max_documents = max_documents
        
        # Scraping avec force refresh si demandÃ©
        if force_refresh:
            logger.info("Mode force refresh : suppression du cache existant")
            scraper.clear_cache()
        
        # ExÃ©cuter le scraping
        documents = await scraper.scrape_all_documentation()
        
        # RÃ©sultats
        result = {
            "status": "success",
            "document_count": len(documents),
            "scraped_at": scraper.get_timestamp(),
            "output_dir": str(scraper.output_dir),
            "force_refresh": force_refresh,
            "documents": [
                {
                    "doc_id": doc.doc_id,
                    "title": doc.title,
                    "url": doc.url,
                    "section": doc.section,
                    "word_count": len(doc.content.split()) if doc.content else 0
                }
                for doc in documents[:10]  # PremiÃ¨res 10 pour les logs
            ]
        }
        
        logger.info(f"âœ… Scraping terminÃ© avec succÃ¨s: {len(documents)} documents")
        logger.info(f"ğŸ“ SauvegardÃ© dans: {scraper.output_dir}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors du scraping: {str(e)}")
        logger.error(f"Type d'erreur: {type(e).__name__}")
        
        # Retourner les informations d'erreur pour le debugging
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "document_count": 0
        }


@task(
    name="validate_scraped_docs",
    description="Valider les documents scrapÃ©s",
    retries=1,
    tags=["validation", "data-quality"]
)
def validate_scraped_docs(scraping_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Valider la qualitÃ© des documents scrapÃ©s
    
    Args:
        scraping_result: RÃ©sultat du scraping prÃ©cÃ©dent
        
    Returns:
        Dict avec les rÃ©sultats de validation
    """
    logger = get_run_logger()
    
    try:
        logger.info("ğŸ” Validation des documents scrapÃ©s")
        
        # VÃ©rifier le statut du scraping
        if scraping_result.get("status") != "success":
            logger.error(f"Scraping en Ã©chec: {scraping_result.get('error_message')}")
            return {
                "status": "failed",
                "reason": "scraping_failed",
                "scraping_error": scraping_result.get("error_message")
            }
        
        document_count = scraping_result.get("document_count", 0)
        
        # CritÃ¨res de validation
        min_documents = 30  # Minimum attendu
        min_avg_words = 100  # Minimum de mots par document
        
        # Validation du nombre de documents
        if document_count < min_documents:
            logger.warning(f"âš ï¸ Nombre de documents insuffisant: {document_count} < {min_documents}")
            return {
                "status": "warning",
                "reason": "insufficient_documents",
                "document_count": document_count,
                "min_expected": min_documents
            }
        
        # Validation de la qualitÃ© des documents
        documents = scraping_result.get("documents", [])
        if documents:
            avg_words = sum(doc.get("word_count", 0) for doc in documents) / len(documents)
            
            if avg_words < min_avg_words:
                logger.warning(f"âš ï¸ QualitÃ© des documents faible: {avg_words:.1f} mots en moyenne")
                return {
                    "status": "warning", 
                    "reason": "low_quality_documents",
                    "avg_word_count": avg_words,
                    "min_expected": min_avg_words
                }
        
        # Validation du rÃ©pertoire de sortie
        output_dir = Path(scraping_result.get("output_dir", ""))
        if not output_dir.exists():
            logger.error(f"âŒ RÃ©pertoire de sortie introuvable: {output_dir}")
            return {
                "status": "failed",
                "reason": "output_directory_missing",
                "output_dir": str(output_dir)
            }
        
        # Compter les fichiers rÃ©ellement crÃ©Ã©s
        md_files = list(output_dir.glob("*.md"))
        json_files = list(output_dir.glob("*.json"))
        
        logger.info(f"ğŸ“„ Fichiers crÃ©Ã©s: {len(md_files)} .md, {len(json_files)} .json")
        
        # Validation rÃ©ussie
        result = {
            "status": "success",
            "document_count": document_count,
            "file_count": {
                "markdown": len(md_files),
                "json": len(json_files)
            },
            "output_dir": str(output_dir),
            "validation_passed": True
        }
        
        logger.info(f"âœ… Validation rÃ©ussie: {document_count} documents valides")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la validation: {str(e)}")
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "validation_passed": False
        }


@task(
    name="check_scraping_freshness",
    description="VÃ©rifier si le scraping est nÃ©cessaire",
    tags=["optimization", "cache"]
)
def check_scraping_freshness(max_age_hours: int = 24) -> Dict[str, Any]:
    """
    VÃ©rifier si les donnÃ©es scrapÃ©es sont rÃ©centes
    
    Args:
        max_age_hours: Age maximum des donnÃ©es en heures
        
    Returns:
        Dict avec les informations de fraÃ®cheur
    """
    logger = get_run_logger()
    
    try:
        from datetime import datetime, timedelta
        import json
        
        # Chemin vers les mÃ©tadonnÃ©es du dernier scraping
        metadata_file = Path("data/raw/scraping_metadata.json")
        
        if not metadata_file.exists():
            logger.info("ğŸ“‹ Aucune mÃ©tadonnÃ©e de scraping trouvÃ©e - scraping nÃ©cessaire")
            return {
                "needs_scraping": True,
                "reason": "no_previous_scraping",
                "last_scraping": None
            }
        
        # Lire les mÃ©tadonnÃ©es
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        last_scraping = datetime.fromisoformat(metadata.get("scraped_at", ""))
        age_hours = (datetime.now() - last_scraping).total_seconds() / 3600
        
        needs_scraping = age_hours > max_age_hours
        
        result = {
            "needs_scraping": needs_scraping,
            "reason": "data_too_old" if needs_scraping else "data_fresh",
            "last_scraping": metadata.get("scraped_at"),
            "age_hours": round(age_hours, 2),
            "max_age_hours": max_age_hours,
            "document_count": metadata.get("document_count", 0)
        }
        
        if needs_scraping:
            logger.info(f"ğŸ”„ Scraping nÃ©cessaire - donnÃ©es vieilles de {age_hours:.1f}h")
        else:
            logger.info(f"âœ… DonnÃ©es rÃ©centes - {age_hours:.1f}h (< {max_age_hours}h)")
        
        return result
        
    except Exception as e:
        logger.warning(f"âš ï¸ Erreur lors de la vÃ©rification: {str(e)}")
        # En cas d'erreur, on recommande le scraping par sÃ©curitÃ©
        return {
            "needs_scraping": True,
            "reason": "check_failed",
            "error": str(e)
        }