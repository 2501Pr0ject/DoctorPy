"""
TÃ¢ches Prefect pour la gÃ©nÃ©ration d'embeddings
"""

import sys
from pathlib import Path
from typing import List, Dict, Any
from prefect import task, get_run_logger

# Ajouter le rÃ©pertoire racine au path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from scripts.data_processing.create_embeddings import EmbeddingGenerator


@task(
    name="create_embeddings",
    description="GÃ©nÃ©rer les embeddings pour les chunks",
    retries=2,
    retry_delay_seconds=[60, 300],
    timeout_seconds=1800,  # 30 minutes max
    tags=["data", "embeddings", "ml"]
)
async def create_embeddings(
    processing_result: Dict[str, Any],
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    batch_size: int = 32,
    force_regenerate: bool = False
) -> Dict[str, Any]:
    """
    GÃ©nÃ©rer les embeddings pour tous les chunks
    
    Args:
        processing_result: RÃ©sultat du traitement prÃ©cÃ©dent
        model_name: Nom du modÃ¨le sentence-transformers
        batch_size: Taille des batches pour le traitement
        force_regenerate: Force la rÃ©gÃ©nÃ©ration mÃªme si les embeddings existent
        
    Returns:
        Dict avec les mÃ©tadonnÃ©es de gÃ©nÃ©ration
    """
    logger = get_run_logger()
    
    try:
        logger.info(f"ğŸ§  DÃ©marrage de la gÃ©nÃ©ration d'embeddings")
        logger.info(f"ModÃ¨le: {model_name}, Batch size: {batch_size}")
        
        # VÃ©rifier le statut du traitement
        if processing_result.get("status") != "success":
            logger.error("âŒ Traitement en Ã©chec - impossible de gÃ©nÃ©rer les embeddings")
            return {
                "status": "failed",
                "reason": "processing_failed",
                "embedding_count": 0
            }
        
        # Initialiser le gÃ©nÃ©rateur d'embeddings
        generator = EmbeddingGenerator(
            model_name=model_name,
            batch_size=batch_size
        )
        
        # Obtenir le rÃ©pertoire des chunks
        chunks_dir = Path(processing_result.get("output_dir", "data/processed"))
        if not chunks_dir.exists():
            logger.error(f"âŒ RÃ©pertoire de chunks introuvable: {chunks_dir}")
            return {
                "status": "failed",
                "reason": "chunks_directory_missing",
                "chunks_dir": str(chunks_dir)
            }
        
        # Force regenerate si demandÃ©
        if force_regenerate:
            logger.info("ğŸ”„ Mode force regenerate : suppression du cache existant")
            generator.clear_cache()
        
        # GÃ©nÃ©ration des embeddings
        logger.info(f"ğŸ“‚ GÃ©nÃ©ration depuis: {chunks_dir}")
        result_data = await generator.generate_all_embeddings(chunks_dir)
        
        # PrÃ©parer le rÃ©sultat
        result = {
            "status": "success",
            "embedding_count": result_data.get("total_embeddings", 0),
            "chunk_count": result_data.get("total_chunks", 0),
            "model_name": model_name,
            "embedding_dimension": result_data.get("embedding_dimension", 384),
            "batch_size": batch_size,
            "generated_at": result_data.get("generation_date"),
            "output_dir": str(result_data.get("output_dir", "data/embeddings")),
            "processing_time": result_data.get("processing_time_seconds", 0),
            "batches_processed": result_data.get("batches_processed", 0),
            "avg_batch_time": result_data.get("avg_batch_time", 0),
            "sections": result_data.get("sections", {})
        }
        
        logger.info(f"âœ… GÃ©nÃ©ration terminÃ©e avec succÃ¨s:")
        logger.info(f"   ğŸ§© Chunks traitÃ©s: {result['chunk_count']}")
        logger.info(f"   ğŸ§  Embeddings crÃ©Ã©s: {result['embedding_count']}")
        logger.info(f"   ğŸ“ Dimension: {result['embedding_dimension']}D")
        logger.info(f"   â±ï¸ Temps de traitement: {result['processing_time']:.1f}s")
        logger.info(f"   ğŸ“ SauvegardÃ© dans: {result['output_dir']}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la gÃ©nÃ©ration d'embeddings: {str(e)}")
        logger.error(f"Type d'erreur: {type(e).__name__}")
        
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "embedding_count": 0
        }


@task(
    name="validate_embeddings",
    description="Valider la qualitÃ© des embeddings gÃ©nÃ©rÃ©s",
    retries=1,
    tags=["validation", "ml"]
)
def validate_embeddings(embedding_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Valider la qualitÃ© des embeddings gÃ©nÃ©rÃ©s
    
    Args:
        embedding_result: RÃ©sultat de la gÃ©nÃ©ration prÃ©cÃ©dente
        
    Returns:
        Dict avec les rÃ©sultats de validation
    """
    logger = get_run_logger()
    
    try:
        logger.info("ğŸ” Validation des embeddings gÃ©nÃ©rÃ©s")
        
        # VÃ©rifier le statut de la gÃ©nÃ©ration
        if embedding_result.get("status") != "success":
            logger.error(f"GÃ©nÃ©ration en Ã©chec: {embedding_result.get('error_message')}")
            return {
                "status": "failed",
                "reason": "generation_failed",
                "generation_error": embedding_result.get("error_message")
            }
        
        embedding_count = embedding_result.get("embedding_count", 0)
        chunk_count = embedding_result.get("chunk_count", 0)
        embedding_dimension = embedding_result.get("embedding_dimension", 0)
        
        # CritÃ¨res de validation
        min_embeddings = 100
        expected_dimension = 384  # Pour all-MiniLM-L6-v2
        
        # Validation du nombre d'embeddings
        if embedding_count < min_embeddings:
            logger.warning(f"âš ï¸ Nombre d'embeddings insuffisant: {embedding_count} < {min_embeddings}")
            return {
                "status": "warning",
                "reason": "insufficient_embeddings",
                "embedding_count": embedding_count,
                "min_expected": min_embeddings
            }
        
        # Validation de la correspondance chunks/embeddings
        if embedding_count != chunk_count:
            logger.warning(f"âš ï¸ Mismatch chunks/embeddings: {chunk_count} chunks, {embedding_count} embeddings")
            return {
                "status": "warning",
                "reason": "chunk_embedding_mismatch",
                "chunk_count": chunk_count,
                "embedding_count": embedding_count
            }
        
        # Validation de la dimension
        if embedding_dimension != expected_dimension:
            logger.warning(f"âš ï¸ Dimension inattendue: {embedding_dimension} != {expected_dimension}")
            return {
                "status": "warning",
                "reason": "unexpected_dimension",
                "actual_dimension": embedding_dimension,
                "expected_dimension": expected_dimension
            }
        
        # Validation du rÃ©pertoire de sortie
        output_dir = Path(embedding_result.get("output_dir", ""))
        if not output_dir.exists():
            logger.error(f"âŒ RÃ©pertoire de sortie introuvable: {output_dir}")
            return {
                "status": "failed",
                "reason": "output_directory_missing",
                "output_dir": str(output_dir)
            }
        
        # Compter les fichiers crÃ©Ã©s
        npy_files = list(output_dir.glob("*.npy"))
        json_files = list(output_dir.glob("*.json"))
        
        if not npy_files:
            logger.error("âŒ Aucun fichier d'embeddings (.npy) trouvÃ©")
            return {
                "status": "failed",
                "reason": "no_embedding_files_found",
                "output_dir": str(output_dir)
            }
        
        # VÃ©rifier la taille des fichiers embeddings
        import numpy as np
        total_embeddings_in_files = 0
        
        for npy_file in npy_files[:5]:  # VÃ©rifier les 5 premiers fichiers
            try:
                embeddings = np.load(npy_file)
                total_embeddings_in_files += len(embeddings)
                
                # VÃ©rifier la dimension
                if len(embeddings.shape) != 2 or embeddings.shape[1] != embedding_dimension:
                    logger.warning(f"âš ï¸ Dimension incorrecte dans {npy_file.name}: {embeddings.shape}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Erreur lors de la lecture de {npy_file.name}: {e}")
        
        # Validation de la qualitÃ© des donnÃ©es
        sections = embedding_result.get("sections", {})
        processing_time = embedding_result.get("processing_time", 0)
        avg_batch_time = embedding_result.get("avg_batch_time", 0)
        
        # Validation rÃ©ussie
        result = {
            "status": "success",
            "embedding_count": embedding_count,
            "chunk_count": chunk_count,
            "embedding_dimension": embedding_dimension,
            "file_count": {
                "embeddings": len(npy_files),
                "metadata": len(json_files)
            },
            "sample_files_checked": min(5, len(npy_files)),
            "performance": {
                "total_time": processing_time,
                "avg_batch_time": avg_batch_time,
                "embeddings_per_second": round(embedding_count / processing_time, 2) if processing_time > 0 else 0
            },
            "sections": list(sections.keys()),
            "output_dir": str(output_dir),
            "validation_passed": True
        }
        
        logger.info(f"âœ… Validation rÃ©ussie:")
        logger.info(f"   ğŸ§  {embedding_count} embeddings valides")
        logger.info(f"   ğŸ“ Dimension: {embedding_dimension}D")
        logger.info(f"   ğŸ“ {len(npy_files)} fichiers crÃ©Ã©s")
        logger.info(f"   âš¡ Performance: {result['performance']['embeddings_per_second']} embeddings/sec")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la validation: {str(e)}")
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "validation_passed": False
        }


@task(
    name="test_embedding_similarity",
    description="Tester la qualitÃ© des embeddings avec des similaritÃ©s",
    tags=["testing", "similarity"]
)
def test_embedding_similarity(embedding_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Tester la qualitÃ© des embeddings en calculant des similaritÃ©s
    
    Args:
        embedding_result: RÃ©sultat de la gÃ©nÃ©ration prÃ©cÃ©dente
        
    Returns:
        Dict avec les rÃ©sultats de test
    """
    logger = get_run_logger()
    
    try:
        logger.info("ğŸ§ª Test de qualitÃ© des embeddings par similaritÃ©")
        
        if embedding_result.get("status") != "success":
            logger.warning("âš ï¸ GÃ©nÃ©ration en Ã©chec - test ignorÃ©")
            return {
                "status": "skipped",
                "reason": "generation_failed"
            }
        
        import numpy as np
        import json
        from sklearn.metrics.pairwise import cosine_similarity
        
        output_dir = Path(embedding_result.get("output_dir", "data/embeddings"))
        
        # Charger un Ã©chantillon d'embeddings pour test
        npy_files = list(output_dir.glob("*.npy"))
        if not npy_files:
            logger.warning("âš ï¸ Aucun fichier d'embeddings trouvÃ© pour le test")
            return {
                "status": "skipped",
                "reason": "no_embedding_files"
            }
        
        # Prendre le premier fichier comme Ã©chantillon
        sample_file = npy_files[0]
        embeddings = np.load(sample_file)
        
        if len(embeddings) < 10:
            logger.warning("âš ï¸ Ã‰chantillon trop petit pour le test")
            return {
                "status": "skipped",
                "reason": "sample_too_small"
            }
        
        # Prendre un Ã©chantillon alÃ©atoire de 10 embeddings
        sample_indices = np.random.choice(len(embeddings), min(10, len(embeddings)), replace=False)
        sample_embeddings = embeddings[sample_indices]
        
        # Calculer les similaritÃ©s cosinus
        similarities = cosine_similarity(sample_embeddings)
        
        # Analyser les rÃ©sultats
        # SimilaritÃ©s avec soi-mÃªme (diagonale) doivent Ãªtre 1.0
        self_similarities = np.diag(similarities)
        
        # SimilaritÃ©s avec les autres (hors diagonale)
        mask = np.ones_like(similarities, dtype=bool)
        np.fill_diagonal(mask, False)
        other_similarities = similarities[mask]
        
        # Calculer les statistiques
        stats = {
            "self_similarity": {
                "mean": float(np.mean(self_similarities)),
                "std": float(np.std(self_similarities)),
                "min": float(np.min(self_similarities)),
                "max": float(np.max(self_similarities))
            },
            "other_similarity": {
                "mean": float(np.mean(other_similarities)),
                "std": float(np.std(other_similarities)),
                "min": float(np.min(other_similarities)),
                "max": float(np.max(other_similarities))
            }
        }
        
        # Ã‰valuation de la qualitÃ©
        quality_issues = []
        quality_score = 100
        
        # Les auto-similaritÃ©s doivent Ãªtre proches de 1.0
        if stats["self_similarity"]["mean"] < 0.99:
            quality_score -= 20
            quality_issues.append("Auto-similaritÃ©s faibles (< 0.99)")
        
        # Les similaritÃ©s croisÃ©es doivent Ãªtre variÃ©es (pas toutes identiques)
        if stats["other_similarity"]["std"] < 0.1:
            quality_score -= 15
            quality_issues.append("Embeddings peu discriminants (std < 0.1)")
        
        # Les similaritÃ©s croisÃ©es ne doivent pas Ãªtre trop Ã©levÃ©es (pas de sur-clustering)
        if stats["other_similarity"]["mean"] > 0.8:
            quality_score -= 10
            quality_issues.append("Embeddings trop similaires (mean > 0.8)")
        
        result = {
            "status": "success",
            "quality_score": quality_score,
            "quality_issues": quality_issues,
            "sample_size": len(sample_embeddings),
            "embedding_dimension": embeddings.shape[1],
            "similarity_stats": stats,
            "tested_file": sample_file.name,
            "test_passed": quality_score >= 70
        }
        
        logger.info(f"ğŸ§ª Test terminÃ©:")
        logger.info(f"   ğŸ¯ Score de qualitÃ©: {quality_score}/100")
        logger.info(f"   ğŸ” Auto-similaritÃ© moyenne: {stats['self_similarity']['mean']:.3f}")
        logger.info(f"   ğŸ“Š SimilaritÃ© croisÃ©e: {stats['other_similarity']['mean']:.3f} Â± {stats['other_similarity']['std']:.3f}")
        
        if quality_issues:
            logger.warning(f"âš ï¸ ProblÃ¨mes dÃ©tectÃ©s: {', '.join(quality_issues)}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors du test: {str(e)}")
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "test_passed": False
        }